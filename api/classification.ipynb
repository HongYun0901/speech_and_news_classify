{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW \n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertModel\n",
    "from transformers import BertConfig\n",
    "\n",
    "##回傳類別\n",
    "def get_tag(str_in):\n",
    "    pretrain_model_path = '/home/nlp/NewsClassify/bert_pretrain_news/'\n",
    "    model_path = '/home/nlp/NewsClassify/'\n",
    "    ##新聞分類\n",
    "    NUM_LABELS = 7\n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrain_model_path)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BertForSequenceClassification.from_pretrained(pretrain_model_path,num_labels = NUM_LABELS)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path+'BERT_for_xgboost_0.pkl'), strict=False)\n",
    "\n",
    "    model.eval()\n",
    "    model1 = BertForSequenceClassification.from_pretrained(pretrain_model_path,num_labels = NUM_LABELS)\n",
    "    model1 = model1.to(device)\n",
    "    model1.load_state_dict(torch.load(model_path+'BERT_for_xgboost_1.pkl'), strict=False)\n",
    "    model1.eval()\n",
    "\n",
    "    model2 = BertForSequenceClassification.from_pretrained(pretrain_model_path,num_labels = NUM_LABELS)\n",
    "    model2 = model2.to(device)\n",
    "    model2.load_state_dict(torch.load(model_path+'BERT_for_xgboost_2.pkl'), strict=False)\n",
    "    model2.eval()\n",
    "    model_list = [model,model1,model2] ##ensemble用\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, input_dict):\n",
    "            self.input_ids = input_dict['input_ids']\n",
    "            self.token_type_ids = input_dict['token_type_ids']\n",
    "            self.attention_mask = input_dict['attention_mask']\n",
    "        def __getitem__(self, idx):\n",
    "            input_id = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "            return input_id, tokentype, attentionmask\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    tag_map = ['政治','生活','國際','體育','娛樂','社會','財經']\n",
    "    score_list = [0,0,0,0,0,0,0]\n",
    "    tag_list = []\n",
    "    threshold = 3\n",
    "    str_in = [str_in] #list\n",
    "    test_input_dict = tokenizer.batch_encode_plus(str_in,\n",
    "                                             add_special_tokens = True,\n",
    "                                             max_length = 512,\n",
    "                                             truncation = True,\n",
    "                                             return_special_tokens_mask = True,\n",
    "                                             pad_to_max_length = True,\n",
    "                                             return_tensors = 'pt')\n",
    "    testset = TestDataset(test_input_dict)\n",
    "    testloader = DataLoader(testset, batch_size = 1)\n",
    "    for data in testloader:\n",
    "        tokens_tensors , segment_tensors,masks_tensors = [t.to(device) for t in data]\n",
    "        for model in model_list:\n",
    "            outputs = model(input_ids = tokens_tensors,\n",
    "                            token_type_ids = segment_tensors,\n",
    "                            attention_mask = masks_tensors)\n",
    "            print(outputs[0][0])\n",
    "            soft = F.Softmax(dim=-1)\n",
    "            test = soft(outputs[0][0])\n",
    "            print(test)\n",
    "            for idx in range(NUM_LABELS):\n",
    "                score_list[idx] += outputs[0][0][idx]\n",
    "        for idx in range(len(score_list)):\n",
    "            if score_list[idx] >= threshold:\n",
    "                tag_list.append(tag_map[idx])\n",
    "    return tag_list  \n",
    "\n",
    "## 回傳報社\n",
    "def get_news(str_in):\n",
    "    pretrain_model_path = '/home/nlp/NewsClassify/bert_pretrain_news/'\n",
    "    model_path = '/home/nlp/NewsClassify/'\n",
    "    ##報社分類\n",
    "    LABELS = 3\n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrain_model_path)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    news_model = BertForSequenceClassification.from_pretrained(pretrain_model_path,num_labels = LABELS)\n",
    "    news_model = news_model.to(device)\n",
    "    news_model.load_state_dict(torch.load(model_path+'BERT_news_2e5_0.pkl'), strict=False)\n",
    "    news_model.eval()\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, input_dict):\n",
    "            self.input_ids = input_dict['input_ids']\n",
    "            self.token_type_ids = input_dict['token_type_ids']\n",
    "            self.attention_mask = input_dict['attention_mask']\n",
    "        def __getitem__(self, idx):\n",
    "            input_id = self.input_ids[idx]\n",
    "            tokentype = self.token_type_ids[idx]\n",
    "            attentionmask = self.attention_mask[idx]\n",
    "            return input_id, tokentype, attentionmask\n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "    tag_map = ['中國時報','自由時報','聯合報']\n",
    "    tag_list = []\n",
    "    str_in = [str_in] #list\n",
    "    test_input_dict = tokenizer.batch_encode_plus(str_in,\n",
    "                                             add_special_tokens = True,\n",
    "                                             max_length = 512,\n",
    "                                             truncation = True,\n",
    "                                             return_special_tokens_mask = True,\n",
    "                                             pad_to_max_length = True,\n",
    "                                             return_tensors = 'pt')\n",
    "    testset = TestDataset(test_input_dict)\n",
    "    testloader = DataLoader(testset, batch_size = 1)\n",
    "    for data in testloader:\n",
    "        tokens_tensors , segment_tensors,masks_tensors = [t.to(device) for t in data]\n",
    "        outputs = news_model(input_ids = tokens_tensors,\n",
    "                        token_type_ids = segment_tensors,\n",
    "                        attention_mask = masks_tensors)\n",
    "        pred_idx = torch.argmax(outputs[0],dim = -1)\n",
    "        tag_list.append(tag_map[pred_idx])\n",
    "    return tag_list  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/nlp/NewsClassify/bert_pretrain_news/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/nlp/NewsClassify/bert_pretrain_news/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /home/nlp/NewsClassify/bert_pretrain_news/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/nlp/NewsClassify/bert_pretrain_news/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /home/nlp/NewsClassify/bert_pretrain_news/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/nlp/NewsClassify/bert_pretrain_news/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3495,  6.2224, -2.0108, -1.8810, -1.1850, -0.9934, -0.3400],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([2.7977e-03, 9.9390e-01, 2.6408e-04, 3.0067e-04, 6.0309e-04, 7.3043e-04,\n",
      "        1.4040e-03], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([-0.1562,  5.6079, -2.4652, -0.6493, -2.2486,  0.8467, -1.2622],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([3.0906e-03, 9.8488e-01, 3.0710e-04, 1.8875e-03, 3.8135e-04, 8.4260e-03,\n",
      "        1.0227e-03], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([ 2.1841,  4.3486, -2.4497, -2.2741, -0.4747, -0.5789,  0.6374],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.0992, 0.8643, 0.0010, 0.0011, 0.0069, 0.0063, 0.0211],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/nlp/NewsClassify/bert_pretrain_news/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/nlp/NewsClassify/bert_pretrain_news/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['生活']\n",
      "['自由時報']\n"
     ]
    }
   ],
   "source": [
    "test = ''\n",
    "ans = get_tag(test)\n",
    "news_ans = get_news(test)\n",
    "print(ans)\n",
    "print(news_ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
