{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import errstate,isneginf\n",
    "from sklearn.preprocessing import normalize\n",
    "from pylab import random\n",
    "import math\n",
    "import jieba\n",
    "\n",
    "file_path = '/home/nlp/NewsClassify/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "民進黨主席卓榮泰晚間召開國際記者會，代表民進黨再次感謝所有台灣人民，再給一次機會，讓民進黨在經歷去年大敗後，再次站穩腳步，國會席次再次過半，也代表了人民對民進黨國會表現的肯定。民進黨達到2020總統連任、國會過半目標。民進黨主席卓榮泰今晚召開國際記者會，代表民進黨再次感謝所有台灣人民，再給民進黨一次機會，讓民進黨在經歷去年大敗後，再次站穩腳步，國會席次再次過半，也代表了人民對民進黨國會表現的肯定。卓榮泰首先向支持蔡英文總統順利當選的國人表達我由衷的感謝。她表示，蔡英文總統連任成功，台灣人民已經用選票向全世界傳遞出三個重要的訊息。第一，我們明確拒絕習近平主席所稱的一國兩制台灣方案，向中國說不。第二，台灣人民捍衛民主制度、保衛國家安全的意志堅定。第三，我們的總統展現強大的領導力，與台灣人民站在一起，守住亞洲民主前線。我們要特別替香港朋友加油，民主的道路，台灣與你們同行。卓榮泰表示，站在世界民主發展來看待台灣此次選舉。當世界的民主面臨嚴峻挑戰、當中國威權擴張滲透世界各國、當境外假訊息排山倒海而來，民進黨必須與台灣人民在這次的選舉過程中，力抗中國共產黨與國內親中勢力的步步進逼，過程十分艱辛，未來挑戰將更多。卓榮泰說，蔡英文總統順利連任，我們除了要感謝台灣人民做我們強而有力的後盾外，也要感謝許多國內外的夥伴、友人對台灣的支持，為台灣發聲。雖然中國在三年內斷了我們七個邦交國，但在外交的防禦上，我們絕對不會放棄持續與世界各國交往，持續與世界組織合作，持續為國際社會貢獻，也誠懇希望國際社會能給予台灣支持，這對台灣將十分重要。此次選舉，觀選團及媒體來得非常多，我們要感謝您們的參與，讓世界看見台灣人民的這次勝利。卓榮泰表示，當然，蔡英文總統的連任，也代表了台灣人民肯定我們的改革路線、年金、稅改、婚姻平權、勞動權益、社會福利等重要的進步路線，台灣社會的進步，將在這些基礎上，持續邁進，特別感謝林全院長、賴清德院長、蘇貞昌院長所帶領的行政團隊，以及蘇嘉全院長和柯建銘總召所帶領的立法院黨團，以及中央黨部全體幹部和各縣市黨公職人員，大家齊心協力，才有我們今天的成果。他接著感謝民進黨第九屆立委結束傑出的表現，國會席次再次過半，也代表了人民對民進黨國會表現的肯定。卓榮泰說，這次區域立委與不分區的提名，民進黨提了一個年輕、多元的名單，讓台灣人民重新對民進黨拾起信任。民進黨將不辜負人民託付，持續栽培年輕人，傳承民主的精神。卓榮泰說，2016年，台灣人民讓民進黨全面執政，國會第一次政黨輪替，我們珍惜這個四年的機會，這場選戰，過程中充滿教訓民進黨的聲音，這樣的聲音我們也都聽到了，我們感謝台灣人民再次給民進黨一個全面執政的機會。未來四年，我們只有更努力，守護我們共同的家，讓我們每一個孩子在一個自由民主的環境裡，不受威脅。最後，我要代表民主進步黨，再次感謝所有台灣人民再給民進黨一次機會，讓民進黨在經歷去年大敗後，再次站穩腳步。身為黨的主席，我完全了解，今天民進黨的勝利，並非代表著大家對民主進步黨完全的信賴，而僅是願意再給我們一次機會而已。卓榮泰說，大家期盼再度完全執政的民進黨，在未來四年，我們要更能貼近社會的各個階層民眾的想法，更能持續讓勇敢堅定的走在壯大台灣的改革之路。他強調，「水低為海、人低無我」，我們絕對不會驕傲，也絕對不會輕忽各位的期待，我們更會更戰戰兢兢，迎接未來的四年。\n",
      "35546\n"
     ]
    }
   ],
   "source": [
    "column_names = ['type','title','text']\n",
    "df = pd.read_csv(file_path+'all_after_mapping.tsv',sep='\\t',names=column_names)\n",
    "doc = df['text'].tolist()\n",
    "print(doc[0])\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.824 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "205720\n"
     ]
    }
   ],
   "source": [
    "# ## 新版\n",
    "# doc_len_list = [] ##doc len\n",
    "# all_doc_data = [] ##all doc split\n",
    "# all_word = {} ##拿來算word_BG的tf值 過濾用\n",
    "# all_word_idf = {}\n",
    "# total_word = 0\n",
    "# ## all doc tf 第一次 建好完整的字典\n",
    "# for i in range(len(doc)):\n",
    "#     if(i%1000 ==0):\n",
    "#         print(i)\n",
    "#     update = {}\n",
    "#     d_content = jieba.cut(doc[i])\n",
    "#     d_content_split = []\n",
    "#     for word in d_content:   ##all word\n",
    "#         d_content_split.append(word)\n",
    "#         if(word in all_word):\n",
    "#             all_word[word] +=1\n",
    "#         else:\n",
    "#             all_word[word] = 1\n",
    "#             all_word_idf[word] = 0\n",
    "#         if word not in update:\n",
    "#             all_word_idf[word] += 1\n",
    "#             update[word] = 1\n",
    "#     all_doc_data.append(d_content_split)\n",
    "# print(len(all_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205720\n"
     ]
    }
   ],
   "source": [
    "## 新版\n",
    "all_doc = []\n",
    "doc_len_list = [] ##doc len\n",
    "doc_dict_list = [] ##紀錄doc出現的所有字\n",
    "all_doc_data = [] ##all doc split\n",
    "all_word = {} ##拿來算word_BG的tf值 過濾用\n",
    "avg_doc_len = 0\n",
    "all_word_idf = {}\n",
    "total_word = 0\n",
    "## all doc tf 第一次 建好完整的字典\n",
    "for i in range(len(doc)):\n",
    "    update = {}\n",
    "    d_content = jieba.cut(doc[i])\n",
    "    d_content_split = [] ## split string\n",
    "    doc_len_list.append(len(doc[i]))\n",
    "    total_word += len(doc[i])\n",
    "    for j in d_content:   ##all word\n",
    "        d_content_split.append(j)\n",
    "        if(j in all_word):\n",
    "            all_word[j] +=1\n",
    "        else:\n",
    "            all_word[j] = 1\n",
    "            all_word_idf[j] = 0\n",
    "        if j not in update:\n",
    "            all_word_idf[j] += 1\n",
    "            update[j] = 1\n",
    "        else:\n",
    "            update[j] += 1 \n",
    "    all_doc.append(d_content)\n",
    "    all_doc_data.append(d_content_split)\n",
    "    doc_dict_list.append(update)\n",
    "print(len(all_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36104\n"
     ]
    }
   ],
   "source": [
    "##過濾 tf值 得到新的dict\n",
    "##過濾透過長度> 2 不含數字 tf>5 出現在95%以下的文章中\n",
    "new_dict = {} ##總共tf>10或是在query裡的字\n",
    "# bg_dict = {}\n",
    "id2word = {}\n",
    "word2id = {} #建完之後對照用的\n",
    "id_count = 0\n",
    "for word in all_word:\n",
    "    if(all_word_idf[word]>=10):\n",
    "        word2id[word] = id_count\n",
    "        id2word[id_count] = word\n",
    "        id_count += 1\n",
    "        new_dict[word] = 0\n",
    "print(len(new_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc = len(doc) ##總共幾篇文章\n",
    "Word = len(new_dict) ##總共幾個字\n",
    "Query = 1 ##總共幾個query\n",
    "R = 5 ##relevant 數量\n",
    "NR = 3 ##最爛幾個Non-relevant\n",
    "EPOCH = 5 ##SMM EM跑幾次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with errstate(divide='ignore'):\n",
    "    def TFIDF():\n",
    "        idf_dict = new_dict.copy()\n",
    "        tf_idf_matrix = np.zeros([Doc+Query,Word]) ##最終分數\n",
    "        count = -1\n",
    "        ## Query TF\n",
    "        for q in range(Query):\n",
    "            query_content = input()\n",
    "            query_tf = new_dict.copy() ##紀錄tf分數\n",
    "            update = {}\n",
    "            query_content = jieba.cut(query_content)\n",
    "            for word in query_content:\n",
    "                if word in new_dict:\n",
    "                    query_tf[word] += 1\n",
    "                    if word not in update:\n",
    "                        idf_dict[word] += 1\n",
    "                        update[word] = 0\n",
    "            tf_idf_matrix[q] = np.array(list(query_tf.values()))\n",
    "        print('Query TF Done!')\n",
    "        ## Doc TF\n",
    "        for d in range(Doc):\n",
    "            doc_content = jieba.cut(doc[d])\n",
    "            doc_tf = new_dict.copy() ##紀錄tf分數\n",
    "            update = {}\n",
    "            for word in doc_content:\n",
    "                if word in new_dict:\n",
    "                    doc_tf[word] += 1\n",
    "                    if word not in update:\n",
    "                        idf_dict[word] += 1\n",
    "                        update[word] = 0\n",
    "            tf_idf_matrix[d+Query] = np.array(list(doc_tf.values()))\n",
    "        print('Doc TF Done!')\n",
    "        ## 剛剛算好的idf 做smooth\n",
    "#         print(idf_dict['intern'])\n",
    "        for key in idf_dict.keys():\n",
    "            idf_dict[key] = math.log(1+((Doc)/(idf_dict[key])))\n",
    "        print('IDF Done!')\n",
    "        ##算出tf-idf分數\n",
    "        idf_array = np.array(list(idf_dict.values()))\n",
    "        for i in range(Doc+Query):\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "            score = np.log(tf_idf_matrix[i])\n",
    "            score += 1\n",
    "            tf_idf_matrix[i] = np.multiply(idf_array,score)\n",
    "        print('TF-IDF Done!')\n",
    "        tf_idf_matrix[isneginf(tf_idf_matrix)]=0\n",
    "        tf_idf_matrix = normalize(tf_idf_matrix,norm ='l2')\n",
    "        print('Norm Done!')\n",
    "        return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "股票 內線\n",
      "Query TF Done!\n",
      "Doc TF Done!\n",
      "IDF Done!\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "TF-IDF Done!\n",
      "Norm Done!\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_matrix = TFIDF()\n",
    "print(tf_idf_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "股票 內線\n"
     ]
    }
   ],
   "source": [
    "idf_dict = new_dict.copy()\n",
    "query_data = []\n",
    "for q in range(Query):\n",
    "    query_content = input()\n",
    "    query_tf = new_dict.copy() ##紀錄tf分數\n",
    "    update = {}\n",
    "    query_content = jieba.cut(query_content)\n",
    "    for word in query_content:\n",
    "        if word in new_dict:\n",
    "            query_data.append(word)\n",
    "            query_tf[word] += 1\n",
    "            if word not in update:\n",
    "                idf_dict[word] += 1\n",
    "                update[word] = 0\n",
    "    tf_idf_matrix[q] = np.array(list(query_tf.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "萬海為增加投資收益，今日公告從2019年7月25日到今日，總計買進中華電股票3,062張，以平均價格108.5652元計算，總交易金額達3.32億元。受到武漢肺炎疫情衝擊，台股開紅盤以來，高低震盪劇烈，但中華電期間跌幅僅1.36%。事實上，萬海陸續買進中華電股票，累計到昨日，萬海總計持有中華電達2.4792萬張，總計金額達23.71億元，持股比例0.31%。\n"
     ]
    }
   ],
   "source": [
    "ans_list = []\n",
    "ans_index_list = []\n",
    "for i in range(Query): ##總共幾個query\n",
    "    query_ans_list = []   ##排序輸出結果\n",
    "    score_list = np.matmul(tf_idf_matrix[Query:],tf_idf_matrix[i]) \n",
    "    result_score = sorted(range(Doc),reverse = True,key = lambda k :score_list[k])\n",
    "    for j in range(Doc):\n",
    "        query_ans_list.append(doc[result_score[j]])\n",
    "    print((query_ans_list[0]))\n",
    "    for k in range(10):\n",
    "        ans_list.append(query_ans_list[k])\n",
    "    ans_index_list.append(result_score) ##relevant index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./result_VSM.txt','w') as f:\n",
    "    for i in range(Query): \n",
    "        for j in range(len(ans_list)):\n",
    "            f.write(str(j+1)+'\\n')\n",
    "            f.write(ans_list[j])\n",
    "            f.write('\\n')\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31807, 21715, 16789, 35520, 1788, 26235, 25223, 3176, 31715, 32487]\n"
     ]
    }
   ],
   "source": [
    "print(ans_index_list[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_BG = new_dict.copy()\n",
    "total = 0\n",
    "for word in word_BG.keys():\n",
    "    word_BG[word] = all_word[word]\n",
    "    total += all_word[word]\n",
    "    \n",
    "for word in word_BG.keys():\n",
    "    word_BG[word] /= total\n",
    "BG = np.array(list(word_BG.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31807, 21715, 16789, 35520, 1788]\n"
     ]
    }
   ],
   "source": [
    "relevant = ans_index_list[0][:R]\n",
    "print(relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_SMM = random([Query,Word])\n",
    "P_Tsmm = random([Query,Word])\n",
    "\n",
    "## norm\n",
    "for i in range(Query):\n",
    "    P_SMM[i] /= sum(P_SMM[i][:])\n",
    "    P_Tsmm[i] /= sum(P_Tsmm[i][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 36104)\n",
      "(1, 36104)\n",
      "(36104,)\n"
     ]
    }
   ],
   "source": [
    "print(P_SMM.shape)\n",
    "print(P_Tsmm.shape)\n",
    "print(BG.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_STEP(P_SMM,P_Tsmm,a):\n",
    "    for i in range(Query):\n",
    "        P_Tsmm[i] = ((1-a) * P_SMM[i]) / (((1-a) * P_SMM[i]) + (a * BG))\n",
    "    ## norm\n",
    "    for i in range(Query):\n",
    "        P_Tsmm[i] /= sum(P_Tsmm[i][:])\n",
    "#     print(P_Tsmm[0])\n",
    "    return P_Tsmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_STEP(P_SMM,P_Tsmm):\n",
    "    P_SMM = np.zeros([Query,Word])\n",
    "    for i in range(Query):\n",
    "        SMM = np.zeros([Word])\n",
    "        for r in range(R):\n",
    "            content = jieba.cut(doc[relevant[r]])\n",
    "            for word in content:\n",
    "                if word in new_dict.keys():\n",
    "                    idx = word2id[word]\n",
    "                    SMM[idx] += doc_dict_list[relevant[r]][word] * P_Tsmm[i][idx]\n",
    "        SMM /= sum(SMM[:])\n",
    "        P_SMM[i] = SMM\n",
    "    return P_SMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.3\n",
    "for e in range(EPOCH):\n",
    "    print(e)\n",
    "    P_Tsmm = E_STEP(P_SMM,P_Tsmm,alpha)\n",
    "    P_SMM = M_STEP(P_SMM,P_Tsmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "all_ans_list = []\n",
    "all_index_list = []\n",
    "a = 0.1\n",
    "b = 0.8\n",
    "c = 0.3\n",
    "for i in range(Query):\n",
    "    print(i)\n",
    "    query_score_list = []\n",
    "    query_vec = np.zeros([Word])\n",
    "    for word in query_data[i]:\n",
    "        if word in new_dict.keys():\n",
    "            idx = word2id[word]\n",
    "            query_vec[idx] += a / len(query_data[i])\n",
    "    query_vec += b * P_SMM[i]\n",
    "    query_vec += (1 - a - b) * BG ## done\n",
    "    for d in range(Doc):\n",
    "        score = 0\n",
    "        doc_vec = (1 - c) * BG\n",
    "        for word in doc_dict_list[d]: ##所有doc的字\n",
    "            if word in new_dict.keys():\n",
    "                idx = word2id[word]\n",
    "                doc_vec[idx] += c * doc_dict_list[d][word] / doc_len_list[d]\n",
    "        score -= np.matmul(query_vec,np.log(doc_vec))\n",
    "        query_score_list.append(score)\n",
    "    query_ans_idx = sorted(range(len(query_score_list)),reverse = False ,key = lambda k : query_score_list[k]) \n",
    "    result_list = []\n",
    "    for j in query_ans_idx:\n",
    "        result_list.append(doc[j])\n",
    "    all_ans_list.append(result_list[:10])\n",
    "    all_index_list.append(query_ans_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(all_ans_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./result_KL.txt','w') as f:\n",
    "    for i in range(Query): \n",
    "        for j in range(len(all_ans_list[0])):\n",
    "            f.write(str(j+1)+'\\n')\n",
    "            f.write(all_ans_list[0][j])\n",
    "            f.write('\\n')\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
